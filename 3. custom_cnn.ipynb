{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badabcc1",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline Summary\n",
    "\n",
    "## 1. Datasets and Models Used\n",
    "   - **Datasets**:\n",
    "     - 4 datasets in total: three individual datasets and one combined dataset.\n",
    "   - **Model**:\n",
    "     - Custom lightweight CNN model with:\n",
    "       - **Convolutional Layers**: Three layers with Batch Normalization, ReLU activation, and MaxPooling for spatial feature extraction.\n",
    "       - **Fully Connected Layers**: Two layers with Batch Normalization and ReLU in the hidden layer.\n",
    "       - **Output Layer**: Final fully connected layer for classification into 9 classes.\n",
    "\n",
    "## 2. Experiment Setup\n",
    "   - **Hyperparameter Ranges**:\n",
    "     - **Batch Size**: [16, 32, 64, 128].\n",
    "     - **Learning Rate**: [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005].\n",
    "     - **Epochs**: [15, 25, 35, 50].\n",
    "   - Selected optimal hyperparameters for each model based on validation performance.\n",
    "\n",
    "## 3. Training and Validation Process\n",
    "   - **Training Loop**:\n",
    "     - Optimized models over multiple epochs for various hyperparameter combinations.\n",
    "     - Logged training and validation accuracy and loss per epoch.\n",
    "     - Tracked total training time for each model.\n",
    "   - **Validation**:\n",
    "     - Monitored performance on validation data to track generalization and prevent overfitting.\n",
    "\n",
    "## 4. Preprocessing Steps\n",
    "   - **Image Enhancements**:\n",
    "     - Applied Median Blur, Basic Sharpening, and Contrast Stretching.\n",
    "     - Used CLAHE (Contrast Limited Adaptive Histogram Equalization) for enhanced contrast.\n",
    "   - **Transformations**:\n",
    "     - Resized images to 64x64.\n",
    "     - Applied Random Horizontal Flip for data augmentation.\n",
    "     - Normalized pixel values to mean `[0.485, 0.456, 0.406]` and standard deviation `[0.229, 0.224, 0.225]`.\n",
    "\n",
    "## 5. Evaluation Metrics and Visualizations\n",
    "   - **Test Set Evaluation**:\n",
    "     - Assessed using Macro-averaged F1 score, precision, recall, and per-class F1 scores.\n",
    "   - **Visualizations**:\n",
    "     - Confusion Matrix for class-wise prediction analysis.\n",
    "     - Classification Report Heatmap with precision, recall, and F1 scores.\n",
    "     - ROC Curves for multi-class AUC (Area Under the Curve) evaluation.\n",
    "\n",
    "## 6. Key Outputs for Each Dataset-Model Combination\n",
    "   - **Training and Validation Curves**:\n",
    "     - Generated and saved plots for training and validation accuracy and loss.\n",
    "   - **Model State Saving**:\n",
    "     - Saved trained model states for potential future use.\n",
    "   - **Detailed Metrics Visualizations**:\n",
    "     - Produced classification reports, confusion matrices, and ROC curves for comprehensive performance analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830e8884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
      "NumPy Version: 1.24.1\n",
      "PyTorch Version: 2.4.1+cu121\n",
      "CUDA is available. PyTorch is using GPU.\n",
      "\n",
      "Number of GPUs available: 1\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 4070\n",
      "  Total Memory: 11.99 GB\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Reserved (Cached): 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Prints the installed versions of Python, NumPy, and PyTorch libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Function to check GPU availability and display memory statistics using PyTorch's CUDA interface\n",
    "def check_gpu_status():\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA is available. PyTorch is using GPU.\\n\")\n",
    "        # Get the number of available GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        # Loop through each GPU and display its details\n",
    "        for gpu_id in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)  # In GB\n",
    "            gpu_memory_cached = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)      # In GB\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024 ** 3)  # In GB\n",
    "            print(f\"\\nGPU {gpu_id}: {gpu_name}\")\n",
    "            print(f\"  Total Memory: {gpu_memory_total:.2f} GB\")\n",
    "            print(f\"  Memory Allocated: {gpu_memory_allocated:.2f} GB\")\n",
    "            print(f\"  Memory Reserved (Cached): {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. PyTorch is using the CPU.\")\n",
    "\n",
    "# Run the GPU status check\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27cc6f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:06:14,796 - Training dataset size: 19063\n",
      "2025-05-08 21:06:14,797 - Validation dataset size: 4037\n",
      "2025-05-08 21:06:14,797 - Test dataset size: 4202\n",
      "2025-05-08 21:06:14,799 - Training with Batch Size: 128, Learning Rate: 0.0001, Epochs: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to index mapping: {'1': 0, '10': 1, '100': 2, '1000': 3, '2': 4, '20': 5, '200': 6, '5': 7, '50': 8, '500': 9}\n",
      "NUM_CLASSES set to: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:07:30,048 - Epoch 1/25 - Training: Loss = 1.7005, Accuracy = 0.4913\n",
      "2025-05-08 21:07:45,550 - Epoch 1/25 - Validation: Loss = 1.4755, Accuracy = 0.5915\n",
      "2025-05-08 21:07:45,551 - Time for epoch 1: 90.75s\n",
      "2025-05-08 21:09:01,472 - Epoch 2/25 - Training: Loss = 1.2240, Accuracy = 0.6998\n",
      "2025-05-08 21:09:16,786 - Epoch 2/25 - Validation: Loss = 1.1688, Accuracy = 0.7023\n",
      "2025-05-08 21:09:16,787 - Time for epoch 2: 91.23s\n",
      "2025-05-08 21:10:31,751 - Epoch 3/25 - Training: Loss = 0.9455, Accuracy = 0.7896\n",
      "2025-05-08 21:10:47,300 - Epoch 3/25 - Validation: Loss = 0.9779, Accuracy = 0.7595\n",
      "2025-05-08 21:10:47,301 - Time for epoch 3: 90.51s\n",
      "2025-05-08 21:12:02,186 - Epoch 4/25 - Training: Loss = 0.7509, Accuracy = 0.8429\n",
      "2025-05-08 21:12:18,015 - Epoch 4/25 - Validation: Loss = 0.7976, Accuracy = 0.8006\n",
      "2025-05-08 21:12:18,015 - Time for epoch 4: 90.71s\n",
      "2025-05-08 21:13:32,513 - Epoch 5/25 - Training: Loss = 0.6096, Accuracy = 0.8759\n",
      "2025-05-08 21:13:47,995 - Epoch 5/25 - Validation: Loss = 0.6877, Accuracy = 0.8288\n",
      "2025-05-08 21:13:47,996 - Time for epoch 5: 89.98s\n",
      "2025-05-08 21:15:02,694 - Epoch 6/25 - Training: Loss = 0.5051, Accuracy = 0.8966\n",
      "2025-05-08 21:15:18,121 - Epoch 6/25 - Validation: Loss = 0.6117, Accuracy = 0.8477\n",
      "2025-05-08 21:15:18,122 - Time for epoch 6: 90.12s\n",
      "2025-05-08 21:16:32,010 - Epoch 7/25 - Training: Loss = 0.4213, Accuracy = 0.9174\n",
      "2025-05-08 21:16:47,488 - Epoch 7/25 - Validation: Loss = 0.5497, Accuracy = 0.8578\n",
      "2025-05-08 21:16:47,489 - Time for epoch 7: 89.37s\n",
      "2025-05-08 21:18:02,124 - Epoch 8/25 - Training: Loss = 0.3594, Accuracy = 0.9308\n",
      "2025-05-08 21:18:17,590 - Epoch 8/25 - Validation: Loss = 0.5012, Accuracy = 0.8662\n",
      "2025-05-08 21:18:17,591 - Time for epoch 8: 90.10s\n",
      "2025-05-08 21:19:34,659 - Epoch 9/25 - Training: Loss = 0.3030, Accuracy = 0.9457\n",
      "2025-05-08 21:19:50,820 - Epoch 9/25 - Validation: Loss = 0.4571, Accuracy = 0.8779\n",
      "2025-05-08 21:19:50,821 - Time for epoch 9: 93.23s\n",
      "2025-05-08 21:21:05,759 - Epoch 10/25 - Training: Loss = 0.2620, Accuracy = 0.9551\n",
      "2025-05-08 21:21:21,043 - Epoch 10/25 - Validation: Loss = 0.4211, Accuracy = 0.8890\n",
      "2025-05-08 21:21:21,043 - Time for epoch 10: 90.22s\n",
      "2025-05-08 21:22:35,515 - Epoch 11/25 - Training: Loss = 0.2242, Accuracy = 0.9646\n",
      "2025-05-08 21:22:50,897 - Epoch 11/25 - Validation: Loss = 0.3943, Accuracy = 0.8945\n",
      "2025-05-08 21:22:50,897 - Time for epoch 11: 89.85s\n",
      "2025-05-08 21:24:06,597 - Epoch 12/25 - Training: Loss = 0.1951, Accuracy = 0.9704\n",
      "2025-05-08 21:24:21,928 - Epoch 12/25 - Validation: Loss = 0.3886, Accuracy = 0.8920\n",
      "2025-05-08 21:24:21,929 - Time for epoch 12: 91.03s\n",
      "2025-05-08 21:25:35,910 - Epoch 13/25 - Training: Loss = 0.1679, Accuracy = 0.9757\n",
      "2025-05-08 21:25:51,280 - Epoch 13/25 - Validation: Loss = 0.3578, Accuracy = 0.8992\n",
      "2025-05-08 21:25:51,281 - Time for epoch 13: 89.35s\n",
      "2025-05-08 21:27:06,476 - Epoch 14/25 - Training: Loss = 0.1466, Accuracy = 0.9820\n",
      "2025-05-08 21:27:21,769 - Epoch 14/25 - Validation: Loss = 0.3421, Accuracy = 0.9007\n",
      "2025-05-08 21:27:21,770 - Time for epoch 14: 90.49s\n",
      "2025-05-08 21:28:35,420 - Epoch 15/25 - Training: Loss = 0.1305, Accuracy = 0.9835\n",
      "2025-05-08 21:28:50,879 - Epoch 15/25 - Validation: Loss = 0.3205, Accuracy = 0.9069\n",
      "2025-05-08 21:28:50,880 - Time for epoch 15: 89.11s\n",
      "2025-05-08 21:30:05,238 - Epoch 16/25 - Training: Loss = 0.1125, Accuracy = 0.9873\n",
      "2025-05-08 21:30:21,230 - Epoch 16/25 - Validation: Loss = 0.3235, Accuracy = 0.9088\n",
      "2025-05-08 21:30:21,231 - Time for epoch 16: 90.35s\n",
      "2025-05-08 21:31:35,602 - Epoch 17/25 - Training: Loss = 0.1014, Accuracy = 0.9894\n",
      "2025-05-08 21:31:51,039 - Epoch 17/25 - Validation: Loss = 0.2968, Accuracy = 0.9170\n",
      "2025-05-08 21:31:51,039 - Time for epoch 17: 89.81s\n",
      "2025-05-08 21:33:06,043 - Epoch 18/25 - Training: Loss = 0.0873, Accuracy = 0.9924\n",
      "2025-05-08 21:33:21,492 - Epoch 18/25 - Validation: Loss = 0.3012, Accuracy = 0.9103\n",
      "2025-05-08 21:33:21,493 - Time for epoch 18: 90.45s\n",
      "2025-05-08 21:34:35,893 - Epoch 19/25 - Training: Loss = 0.0785, Accuracy = 0.9930\n",
      "2025-05-08 21:34:52,007 - Epoch 19/25 - Validation: Loss = 0.2953, Accuracy = 0.9126\n",
      "2025-05-08 21:34:52,008 - Time for epoch 19: 90.51s\n",
      "2025-05-08 21:36:06,120 - Epoch 20/25 - Training: Loss = 0.0697, Accuracy = 0.9947\n",
      "2025-05-08 21:36:21,452 - Epoch 20/25 - Validation: Loss = 0.2799, Accuracy = 0.9190\n",
      "2025-05-08 21:36:21,452 - Time for epoch 20: 89.44s\n",
      "2025-05-08 21:37:35,668 - Epoch 21/25 - Training: Loss = 0.0599, Accuracy = 0.9963\n",
      "2025-05-08 21:37:51,075 - Epoch 21/25 - Validation: Loss = 0.2634, Accuracy = 0.9185\n",
      "2025-05-08 21:37:51,075 - Time for epoch 21: 89.62s\n",
      "2025-05-08 21:39:05,878 - Epoch 22/25 - Training: Loss = 0.0539, Accuracy = 0.9971\n",
      "2025-05-08 21:39:21,137 - Epoch 22/25 - Validation: Loss = 0.2790, Accuracy = 0.9113\n",
      "2025-05-08 21:39:21,138 - Time for epoch 22: 90.06s\n",
      "2025-05-08 21:40:35,414 - Epoch 23/25 - Training: Loss = 0.0488, Accuracy = 0.9973\n",
      "2025-05-08 21:40:50,815 - Epoch 23/25 - Validation: Loss = 0.2648, Accuracy = 0.9183\n",
      "2025-05-08 21:40:50,816 - Time for epoch 23: 89.68s\n",
      "2025-05-08 21:42:06,585 - Epoch 24/25 - Training: Loss = 0.0424, Accuracy = 0.9981\n",
      "2025-05-08 21:42:22,225 - Epoch 24/25 - Validation: Loss = 0.2793, Accuracy = 0.9155\n",
      "2025-05-08 21:42:22,226 - Time for epoch 24: 91.41s\n",
      "2025-05-08 21:43:35,733 - Epoch 25/25 - Training: Loss = 0.0376, Accuracy = 0.9985\n",
      "2025-05-08 21:43:51,114 - Epoch 25/25 - Validation: Loss = 0.2591, Accuracy = 0.9242\n",
      "2025-05-08 21:43:51,115 - Time for epoch 25: 88.89s\n",
      "2025-05-08 21:43:51,115 - Total Training Time: 2256.28s\n",
      "2025-05-08 21:44:07,456 - Macro-Averaged F1 Score: 0.8955\n"
     ]
    }
   ],
   "source": [
    "# Image classification pipeline using a custom simple CNN model and enhanced preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "train_dir = 'dataset/dataset_aug/train'\n",
    "val_dir = 'dataset/dataset_aug/val'\n",
    "test_dir = 'dataset/dataset_aug/test'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.0001]\n",
    "epoch_counts = [25]\n",
    "NUM_CLASSES = len(datasets.ImageFolder(train_dir).classes)\n",
    "print(\"Class to index mapping:\", datasets.ImageFolder(train_dir).class_to_idx)\n",
    "print(f\"NUM_CLASSES set to: {NUM_CLASSES}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        return cv2.medianBlur(image, 3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in median blur: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        image = image.astype(np.uint8)\n",
    "        return cv2.filter2D(image, -1, kernel)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in basic sharpen: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image_float = image.astype(float)\n",
    "        for i in range(3):\n",
    "            p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "            image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "        return image_float.astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in contrast stretch: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Custom CNN Model Definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            # Debugging: check label range\n",
    "            if not torch.all((labels >= 0) & (labels < NUM_CLASSES)):\n",
    "                print(f\"Invalid label(s) found in batch: {labels}\")\n",
    "                print(f\"Unique labels in batch: {labels.unique()}\")\n",
    "                raise ValueError(f\"Invalid label detected. Expected labels between 0 and {NUM_CLASSES - 1}.\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.cpu())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "        logger.info(f\"Time for epoch {epoch + 1}: {epoch_time:.2f}s\")\n",
    "\n",
    "    logger.info(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "def test_and_evaluate(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate and log macro F1 score\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Create classification metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    metrics_df = np.array([precision, recall, f1]).T\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap=\"viridis\", xticklabels=[\"Precision\", \"Recall\", \"F1\"], yticklabels=class_names)\n",
    "    plt.title(\"Dataset CustomCNN Classification Report\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.savefig('dataset_customcnn_classification_report.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Dataset CustomCNN Confusion Matrix\")\n",
    "    plt.savefig('dataset_customcnn_confusion_matrix.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create ROC curves\n",
    "    all_labels_binarized = label_binarize(all_labels, classes=np.arange(NUM_CLASSES))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Dataset CustomCNN ROC Curves for All Classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('dataset_customcnn_ROC_All_Classes.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Training and Evaluation Pipeline\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epoch_counts:\n",
    "            logger.info(f\"Training with Batch Size: {batch_size}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = CustomCNN()\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_and_validate(\n",
    "                model, train_loader, val_loader, criterion, optimizer, epochs\n",
    "            )\n",
    "\n",
    "            # Save accuracy and loss graphs\n",
    "            epochs_range = range(1, epochs + 1)\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Dataset CustomCNN Accuracy (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_customcnn_accuracy_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Dataset CustomCNN Loss (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_customcnn_loss_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            # Test and evaluate model\n",
    "            test_and_evaluate(model, test_loader, class_names=test_dataset.classes)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), 'dataset_customcnn_model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48b01e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 286794\n",
      "Trainable parameters: 286794\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your defined PyTorch model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total parameters: {total_params}')\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ef77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function for a single image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    image = image.astype(np.uint8)\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "    return cv2.medianBlur(image, 3)\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    image = image.astype(np.uint8)\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    image_float = image.astype(float)\n",
    "    for i in range(3):\n",
    "        p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "        image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "    return image_float.astype(np.uint8)\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "    transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "    transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "    CLAHE(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def classify_note_image(image_path, model_weights_path, class_names):\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Load the model & weights\n",
    "    model = CustomCNN(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "    predicted_label = class_names[predicted_idx.item()]\n",
    "    confidence = probabilities[0][predicted_idx].item()\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_label} (Confidence: {confidence:.2f})\")\n",
    "    return predicted_label, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64668309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rezaul Karim\\AppData\\Local\\Temp\\ipykernel_14128\\3934531084.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 20 (Confidence: 0.97)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('20', 0.9661031365394592)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define your class labels based on your dataset\n",
    "class_names = ['1', '10', '100','1000', '2', '20', '200', '5', '50', '500']\n",
    "\n",
    "# Call the function with your image\n",
    "image_path = 'fig_20_torn_note.png'\n",
    "model_weights_path = 'dataset_customcnn_model_trained.pth'\n",
    "classify_note_image(image_path, model_weights_path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb32f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0, '10': 1, '100': 2, '1000': 3, '2': 4, '20': 5, '200': 6, '5': 7, '50': 8, '500': 9}\n"
     ]
    }
   ],
   "source": [
    "print(datasets.ImageFolder(train_dir).class_to_idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
