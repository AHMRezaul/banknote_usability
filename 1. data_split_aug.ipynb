{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479bd576",
   "metadata": {},
   "source": [
    "# Data Splitting and Data Augmentation Summary\n",
    "\n",
    "## 1. Data Splitting Process\n",
    "   - Split each dataset into train, validation, and test sets using multiple ratios: 60/20/20, 70/15/15, and 80/10/10.\n",
    "   - The splitting was performed on all three individual datasets as well as the combined dataset.\n",
    "   - Ensured consistent directory structure and proper allocation across splits.\n",
    "   - **Details**:\n",
    "     - Utilized a shuffling mechanism to ensure randomness in split allocation.\n",
    "     - **Directory Structure**:\n",
    "       - Created `train`, `val`, and `test` directories for each class in each dataset split.\n",
    "   - **Outcome**:\n",
    "     - Generated separate training, validation, and testing datasets for different split ratios.\n",
    "     - Provided detailed statistics for per-class and overall splits.\n",
    "\n",
    "## 2. Data Augmentation Process\n",
    "   - Applied data augmentations to the pre-split datasets across all splits (60/20/20, 70/15/15, and 80/10/10).\n",
    "   - Augmentation was performed independently for train, validation, and test splits.\n",
    "   - **Augmentation Techniques**:\n",
    "     - **Transformations Used**:\n",
    "       - `RandomResizedCrop`: Randomly resized crops of images.\n",
    "       - `RandomRotation`: Applied random rotations.\n",
    "       - `RandomHorizontalFlip`: Randomly flipped images horizontally.\n",
    "       - `ColorJitter`: Adjusted brightness, contrast, saturation, and hue.\n",
    "       - `RandomAffine`: Applied random affine transformations.\n",
    "       - `RandomErasing`: Performed random erasing for data augmentation.\n",
    "     - **Augmentation Count**:\n",
    "       - Generated 10 augmented images per original image.\n",
    "   - **Outcome**:\n",
    "     - Created enhanced datasets with multiple augmentations per original image across different splits and combinations.\n",
    "     - Detailed statistics and summaries provided for augmented data.\n",
    "\n",
    "## 3. Tools and Libraries Utilized\n",
    "   - **Data Splitting**: Utilized Python's `os`, `shutil`, and `random` libraries for file handling and directory management.\n",
    "   - **Image Augmentation**: Used `PIL` for image handling and `torchvision.transforms` for augmentations.\n",
    "   - **Progress Monitoring**: Employed `tqdm` for tracking file operations and augmentation processes.\n",
    "\n",
    "## 4. Final Results\n",
    "   - Delivered train, validation, and test splits with consistent class distribution across various split ratios.\n",
    "   - Generated augmented datasets with comprehensive transformations to increase data diversity.\n",
    "   - Processed all three individual datasets as well as the combined dataset.\n",
    "   - Provided detailed documentation and summaries, including per-class statistics and overall dataset statistics for each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8b294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.4.1%2Bcu118-cp38-cp38-win_amd64.whl (2695.5 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp38-cp38-win_amd64.whl (5.0 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp38-cp38-win_amd64.whl (4.0 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\study\\spring 25\\cs682 computer vision\\project\\project_env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.24.1-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 3.9/14.9 MB 21.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.7/14.9 MB 22.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 13.6/14.9 MB 23.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.9/14.9 MB 22.2 MB/s eta 0:00:00\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-10.2.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 18.3 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 22.8 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 5.8/6.2 MB 27.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 22.4 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.0 numpy-1.24.1 pillow-10.2.0 sympy-1.13.3 torch-2.4.1+cu118 torchaudio-2.4.1+cu118 torchvision-0.19.1+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (e:\\study\\spring 25\\cs682 computer vision\\project\\project_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (e:\\study\\spring 25\\cs682 computer vision\\project\\project_env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (e:\\study\\spring 25\\cs682 computer vision\\project\\project_env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# # Create the virtual environment named 'dmp'\n",
    "# !python3 -m venv /scratch/movi/dmp\n",
    "# Install ipykernel inside the 'dmp' environment\n",
    "# !pip install ipykernel\n",
    "# Add 'dmp' as a kernel for Jupyter Notebook\n",
    "# !python -m ipykernel install --user --name=dmp --display-name \"Python (dmp)\"\n",
    "# # Upgrade pip in the 'dmp' environment\n",
    "# !pip install --upgrade pip\n",
    "# # Install necessary packages (NumPy, PyTorch, etc.) inside 'dmp'\n",
    "# !pip install numpy torch torchvision torchaudio pandas matplotlib scikit-learn\n",
    "# For CUDA 11.8\n",
    "# !pip uninstall torch torchvision torchaudio\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install numpy==1.21.4 scikit-learn==1.0.2\n",
    "# import tensorflow as tf\n",
    "# print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2a9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1+cu118\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fef238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.9 (tags/v3.8.9:a743f81, Apr  6 2021, 14:02:34) [MSC v.1928 64 bit (AMD64)]\n",
      "NumPy Version: 1.24.1\n",
      "PyTorch Version: 2.4.1+cu118\n",
      "CUDA is available. PyTorch is using GPU.\n",
      "\n",
      "Number of GPUs available: 1\n",
      "\n",
      "GPU 0: NVIDIA GeForce RTX 4070\n",
      "  Total Memory: 11.99 GB\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Reserved (Cached): 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Prints the installed versions of Python, NumPy, and PyTorch libraries\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to check GPU availability and display memory statistics using PyTorch's CUDA interface\n",
    "\n",
    "def check_gpu_status():\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA is available. PyTorch is using GPU.\\n\")\n",
    "\n",
    "        # Get the number of available GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "        # Loop through each GPU and display its details\n",
    "        for gpu_id in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)  # In GB\n",
    "            gpu_memory_cached = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)      # In GB\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024 ** 3)  # In GB\n",
    "\n",
    "            print(f\"\\nGPU {gpu_id}: {gpu_name}\")\n",
    "            print(f\"  Total Memory: {gpu_memory_total:.2f} GB\")\n",
    "            print(f\"  Memory Allocated: {gpu_memory_allocated:.2f} GB\")\n",
    "            print(f\"  Memory Reserved (Cached): {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. PyTorch is using the CPU.\")\n",
    "\n",
    "# Run the GPU status check\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddc9c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 25 train, 5 val, 6 test (Total: 36)\n",
      "10: 248 train, 53 val, 54 test (Total: 355)\n",
      "100: 172 train, 36 val, 38 test (Total: 246)\n",
      "1000: 175 train, 37 val, 38 test (Total: 250)\n",
      "2: 184 train, 39 val, 41 test (Total: 264)\n",
      "20: 253 train, 54 val, 55 test (Total: 362)\n",
      "200: 13 train, 2 val, 4 test (Total: 19)\n",
      "5: 207 train, 44 val, 46 test (Total: 297)\n",
      "50: 248 train, 53 val, 54 test (Total: 355)\n",
      "500: 208 train, 44 val, 46 test (Total: 298)\n",
      "\n",
      "Overall Dataset Summary:\n",
      "Total Images: 2482\n",
      "Train Ratio: 0.7, Validation Ratio: 0.15, Test Ratio: 0.15\n",
      "\n",
      "Detailed Split Summary:\n",
      "1 - Total: 36, Train: 25, Val: 5, Test: 6\n",
      "10 - Total: 355, Train: 248, Val: 53, Test: 54\n",
      "100 - Total: 246, Train: 172, Val: 36, Test: 38\n",
      "1000 - Total: 250, Train: 175, Val: 37, Test: 38\n",
      "2 - Total: 264, Train: 184, Val: 39, Test: 41\n",
      "20 - Total: 362, Train: 253, Val: 54, Test: 55\n",
      "200 - Total: 19, Train: 13, Val: 2, Test: 4\n",
      "5 - Total: 297, Train: 207, Val: 44, Test: 46\n",
      "50 - Total: 355, Train: 248, Val: 53, Test: 54\n",
      "500 - Total: 298, Train: 208, Val: 44, Test: 46\n"
     ]
    }
   ],
   "source": [
    "# Code to split dataset into train, validation, and test sets, with detailed analysis and logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset = 'dataset/dataset_combined_unique'  # Replace with the path to your original dataset\n",
    "split_base_dir = 'dataset/dataset_split'        # Base directory to store train/val/test splits\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "def create_dir_structure(base_dir, class_names):\n",
    "    \"\"\"Create train, val, and test directories for each class.\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "def analyze_and_split_dataset(original_dataset, split_base_dir):\n",
    "    \"\"\"Analyze dataset and split into train, val, and test sets.\"\"\"\n",
    "    class_names = sorted(os.listdir(original_dataset))  # Get class names in alphabetical order\n",
    "    create_dir_structure(split_base_dir, class_names)   # Create the necessary directory structure\n",
    "\n",
    "    total_images = 0  # Track the total number of images across all classes\n",
    "    split_summary = {}  # Dictionary to store per-class split details\n",
    "\n",
    "    # Loop through each class folder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(original_dataset, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):  # Ensure it's a folder\n",
    "            # List all images in the class folder\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            random.shuffle(image_files)  # Shuffle images to ensure randomness\n",
    "\n",
    "            # Calculate split indices\n",
    "            total_images_in_class = len(image_files)\n",
    "            train_end = int(total_images_in_class * TRAIN_RATIO)\n",
    "            val_end = train_end + int(total_images_in_class * VAL_RATIO)\n",
    "\n",
    "            # Split the image files into train, val, and test sets\n",
    "            train_files = image_files[:train_end]\n",
    "            val_files = image_files[train_end:val_end]\n",
    "            test_files = image_files[val_end:]\n",
    "\n",
    "            # Copy files to the respective split directories\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'train', class_name, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'val', class_name, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'test', class_name, file))\n",
    "\n",
    "            # Store the split summary for this class\n",
    "            split_summary[class_name] = {\n",
    "                'Total': total_images_in_class,\n",
    "                'Train': len(train_files),\n",
    "                'Validation': len(val_files),\n",
    "                'Test': len(test_files)\n",
    "            }\n",
    "\n",
    "            # Update total image count\n",
    "            total_images += total_images_in_class\n",
    "\n",
    "            # Print per-class summary\n",
    "            print(f\"{class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test (Total: {total_images_in_class})\")\n",
    "\n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Dataset Summary:\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Train Ratio: {TRAIN_RATIO}, Validation Ratio: {VAL_RATIO}, Test Ratio: {TEST_RATIO}\\n\")\n",
    "\n",
    "    # Print detailed split summary for all classes\n",
    "    print(\"Detailed Split Summary:\")\n",
    "    for class_name, counts in split_summary.items():\n",
    "        print(f\"{class_name} - Total: {counts['Total']}, Train: {counts['Train']}, Val: {counts['Validation']}, Test: {counts['Test']}\")\n",
    "\n",
    "    return split_summary\n",
    "\n",
    "# Run the split function and store the summary\n",
    "dataset_summary = analyze_and_split_dataset(original_dataset, split_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e966bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Starting Augmentation Process ===\n",
      "\n",
      "Processing TRAIN split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 2379.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:01<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 248 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 248/248 [00:00<00:00, 2582.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 248/248 [00:12<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 172 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 172/172 [00:00<00:00, 2547.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 172/172 [00:09<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 175 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 175/175 [00:00<00:00, 2243.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 175/175 [00:09<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 184 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 184/184 [00:00<00:00, 1999.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 184/184 [00:10<00:00, 18.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 253 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 253/253 [00:00<00:00, 2555.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 253/253 [00:13<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 1999.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:00<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 207 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 207/207 [00:00<00:00, 1815.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 207/207 [00:11<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 248 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 248/248 [00:00<00:00, 2455.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 248/248 [00:13<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 208 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 208/208 [00:00<00:00, 2513.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 208/208 [00:10<00:00, 19.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VAL split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 5 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 5/5 [00:00<00:00, 1998.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 5/5 [00:00<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 53 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 53/53 [00:00<00:00, 2254.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 53/53 [00:02<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 36 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 36/36 [00:00<00:00, 1845.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 36/36 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 37 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 37/37 [00:00<00:00, 2386.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 37/37 [00:01<00:00, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 39 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 39/39 [00:00<00:00, 2599.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 39/39 [00:01<00:00, 19.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 54 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 54/54 [00:00<00:00, 2399.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 54/54 [00:02<00:00, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 2 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 2/2 [00:00<00:00, 1334.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 2/2 [00:00<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 44 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 44/44 [00:00<00:00, 2512.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 44/44 [00:02<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 53 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 53/53 [00:00<00:00, 2119.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 53/53 [00:02<00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 44 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 44/44 [00:00<00:00, 2512.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 44/44 [00:02<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TEST split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 6 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 6/6 [00:00<00:00, 1997.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 6/6 [00:00<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 54 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 54/54 [00:00<00:00, 2249.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 54/54 [00:03<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 38 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 38/38 [00:00<00:00, 2302.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 38/38 [00:02<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 38 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 38/38 [00:00<00:00, 2620.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 38/38 [00:01<00:00, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 41 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 41/41 [00:00<00:00, 2516.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 41/41 [00:02<00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 55 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 55/55 [00:00<00:00, 2000.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 55/55 [00:02<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 4 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 4/4 [00:00<00:00, 2001.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 4/4 [00:00<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 46 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 46/46 [00:00<00:00, 2555.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 46/46 [00:02<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 54 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 54/54 [00:00<00:00, 2347.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 54/54 [00:02<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 46 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 46/46 [00:00<00:00, 2421.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 46/46 [00:02<00:00, 17.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Summary ===\n",
      "\n",
      "TRAIN Split:\n",
      "Class 1:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 10:\n",
      "  Original: 248\n",
      "  Augmented: 2480\n",
      "  Total: 2728\n",
      "Class 100:\n",
      "  Original: 172\n",
      "  Augmented: 1720\n",
      "  Total: 1892\n",
      "Class 1000:\n",
      "  Original: 175\n",
      "  Augmented: 1750\n",
      "  Total: 1925\n",
      "Class 2:\n",
      "  Original: 184\n",
      "  Augmented: 1840\n",
      "  Total: 2024\n",
      "Class 20:\n",
      "  Original: 253\n",
      "  Augmented: 2530\n",
      "  Total: 2783\n",
      "Class 200:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 5:\n",
      "  Original: 207\n",
      "  Augmented: 2070\n",
      "  Total: 2277\n",
      "Class 50:\n",
      "  Original: 248\n",
      "  Augmented: 2480\n",
      "  Total: 2728\n",
      "Class 500:\n",
      "  Original: 208\n",
      "  Augmented: 2080\n",
      "  Total: 2288\n",
      "--------------------------------------------------\n",
      "\n",
      "VAL Split:\n",
      "Class 1:\n",
      "  Original: 5\n",
      "  Augmented: 50\n",
      "  Total: 55\n",
      "Class 10:\n",
      "  Original: 53\n",
      "  Augmented: 530\n",
      "  Total: 583\n",
      "Class 100:\n",
      "  Original: 36\n",
      "  Augmented: 360\n",
      "  Total: 396\n",
      "Class 1000:\n",
      "  Original: 37\n",
      "  Augmented: 370\n",
      "  Total: 407\n",
      "Class 2:\n",
      "  Original: 39\n",
      "  Augmented: 390\n",
      "  Total: 429\n",
      "Class 20:\n",
      "  Original: 54\n",
      "  Augmented: 540\n",
      "  Total: 594\n",
      "Class 200:\n",
      "  Original: 2\n",
      "  Augmented: 20\n",
      "  Total: 22\n",
      "Class 5:\n",
      "  Original: 44\n",
      "  Augmented: 440\n",
      "  Total: 484\n",
      "Class 50:\n",
      "  Original: 53\n",
      "  Augmented: 530\n",
      "  Total: 583\n",
      "Class 500:\n",
      "  Original: 44\n",
      "  Augmented: 440\n",
      "  Total: 484\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST Split:\n",
      "Class 1:\n",
      "  Original: 6\n",
      "  Augmented: 60\n",
      "  Total: 66\n",
      "Class 10:\n",
      "  Original: 54\n",
      "  Augmented: 540\n",
      "  Total: 594\n",
      "Class 100:\n",
      "  Original: 38\n",
      "  Augmented: 380\n",
      "  Total: 418\n",
      "Class 1000:\n",
      "  Original: 38\n",
      "  Augmented: 380\n",
      "  Total: 418\n",
      "Class 2:\n",
      "  Original: 41\n",
      "  Augmented: 410\n",
      "  Total: 451\n",
      "Class 20:\n",
      "  Original: 55\n",
      "  Augmented: 550\n",
      "  Total: 605\n",
      "Class 200:\n",
      "  Original: 4\n",
      "  Augmented: 40\n",
      "  Total: 44\n",
      "Class 5:\n",
      "  Original: 46\n",
      "  Augmented: 460\n",
      "  Total: 506\n",
      "Class 50:\n",
      "  Original: 54\n",
      "  Augmented: 540\n",
      "  Total: 594\n",
      "Class 500:\n",
      "  Original: 46\n",
      "  Augmented: 460\n",
      "  Total: 506\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Dataset Statistics:\n",
      "Total Original Images: 2482\n",
      "Total Augmented Images: 24820\n",
      "Total Images: 27302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths - update these paths\n",
    "split_base_dir = 'dataset/dataset_split'  # Your already split dataset\n",
    "augmented_data_dir = 'dataset/dataset_aug'  # Where to save augmented data\n",
    "\n",
    "# Number of augmentations per image\n",
    "NUM_AUGMENTATIONS = 10\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define augmentation transformations (tensor-based)\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "def apply_augmentations():\n",
    "    print(\"\\n=== Starting Augmentation Process ===\")\n",
    "\n",
    "    # Create destination directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(augmented_data_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for class_name in os.listdir(os.path.join(split_base_dir, split)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "    stats = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split:\")\n",
    "        split_source = os.path.join(split_base_dir, split)\n",
    "        split_dest = os.path.join(augmented_data_dir, split)\n",
    "\n",
    "        for class_name in sorted(os.listdir(split_source)):\n",
    "            class_source = os.path.join(split_source, class_name)\n",
    "            class_dest = os.path.join(split_dest, class_name)\n",
    "\n",
    "            if os.path.isdir(class_source):\n",
    "                original_files = [f for f in os.listdir(class_source) \n",
    "                                  if os.path.isfile(os.path.join(class_source, f))]\n",
    "\n",
    "                print(f\"\\nProcessing class {class_name}:\")\n",
    "                print(f\"Found {len(original_files)} original images\")\n",
    "\n",
    "                # Copy originals\n",
    "                for file in tqdm(original_files, desc=\"Copying originals\"):\n",
    "                    shutil.copy2(os.path.join(class_source, file),\n",
    "                                 os.path.join(class_dest, file))\n",
    "\n",
    "                # Generate augmentations\n",
    "                print(\"Generating augmented images...\")\n",
    "                for file in tqdm(original_files, desc=\"Generating augmentations\"):\n",
    "                    img_path = os.path.join(class_source, file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            # Convert to RGB if needed\n",
    "                            if img.mode != 'RGB':\n",
    "                                img = img.convert('RGB')\n",
    "\n",
    "                            # Convert to tensor and send to GPU\n",
    "                            img_tensor = transforms.ToTensor()(img).to(device)\n",
    "\n",
    "                            for i in range(NUM_AUGMENTATIONS):\n",
    "                                try:\n",
    "                                    # Add batch dimension\n",
    "                                    img_aug = img_tensor.unsqueeze(0)\n",
    "\n",
    "                                    # Apply augmentation (keep on GPU)\n",
    "                                    augmented_tensor = augmentation_transforms(img_aug[0])\n",
    "\n",
    "                                    # Move back to CPU and save\n",
    "                                    augmented_img = transforms.ToPILImage()(augmented_tensor.cpu())\n",
    "\n",
    "                                    base_name = os.path.splitext(file)[0]\n",
    "                                    aug_name = f\"{base_name}_aug_{i+1}.jpg\"\n",
    "                                    augmented_img.save(os.path.join(class_dest, aug_name))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error generating augmentation {i+1} for {file}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file}: {str(e)}\")\n",
    "\n",
    "                total_augmented = len(original_files) * NUM_AUGMENTATIONS\n",
    "                stats[split][class_name] = {\n",
    "                    'original': len(original_files),\n",
    "                    'augmented': total_augmented,\n",
    "                    'total': len(original_files) + total_augmented\n",
    "                }\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== Augmentation Summary ===\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        for class_name, counts in sorted(stats[split].items()):\n",
    "            print(f\"Class {class_name}:\")\n",
    "            print(f\"  Original: {counts['original']}\")\n",
    "            print(f\"  Augmented: {counts['augmented']}\")\n",
    "            print(f\"  Total: {counts['total']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    total_orig = sum(sum(c['original'] for c in s.values()) for s in stats.values())\n",
    "    total_aug = sum(sum(c['augmented'] for c in s.values()) for s in stats.values())\n",
    "    print(\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"Total Original Images: {total_orig}\")\n",
    "    print(f\"Total Augmented Images: {total_aug}\")\n",
    "    print(f\"Total Images: {total_orig + total_aug}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_augmentations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1845256",
   "metadata": {},
   "source": [
    "# END of Data Split and Augmentation\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
